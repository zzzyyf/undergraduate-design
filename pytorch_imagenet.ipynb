{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "metadata": {
    "interpreter": {
     "hash": "2210c89a2d8edf41a854aaf14627c074dbd802e9f484e18ccb89db7185c2e4ea"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "import os\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = '/home/zyf/Documents/ImageNet'\n",
    "transform = transforms.Compose(\n",
    "            [\n",
    "            # scale and normalize to inception_v3 format\n",
    "            transforms.Resize((299, 299)), \n",
    "            transforms.ToTensor(), \n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                                std=[0.229, 0.224, 0.225])\n",
    "            ]) \n",
    "dataset = datasets.ImageNet(root=datapath, split=\"val\", transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "model = models.inception_v3(pretrained=True, progress=False)\n",
    "model = model.eval()\n",
    "model = model.to(device)\n",
    "# print(torch.cuda.memory_summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.benchmark as benchmark\n",
    "num_threads = torch.get_num_threads()\n",
    "\n",
    "repeat_times = 1\n",
    "\n",
    "def inference(model, dataloader, device, enabled):\n",
    "    sum_input = 0\n",
    "    sum_right = 0\n",
    "    for i, (images, targets) in enumerate(dataloader):\n",
    "        # print(f'batch No.{i+1} start!')\n",
    "        with torch.no_grad():\n",
    "            with torch.cuda.amp.autocast(enabled=enabled):\n",
    "                images = images.to(device)\n",
    "                # print(torch.cuda.memory_summary())\n",
    "                outputs = model(images)\n",
    "            \n",
    "                # get accuracy\n",
    "                #'''\n",
    "                predictions = torch.max(outputs, 1)[1]\n",
    "                batch_size = outputs.size(0)\n",
    "                for j in range(batch_size):\n",
    "                    if predictions[j]==targets[j]:\n",
    "                        sum_right += 1\n",
    "                sum_input += batch_size\n",
    "                #'''\n",
    "                #if i==0:\n",
    "                #    break\n",
    "    print(f'evaluated {sum_input} samples, top-1 accuracy: {sum_right * 1.0 / sum_input}')        \n",
    "\n",
    "with open(f'pytorch_fp32_amp_result_{repeat_times}.txt', 'w') as result:\n",
    "    result.write(\n",
    "'''\n",
    "pytorch 1.8.0\n",
    "cuda 11.0\n",
    "python 3.8.5\n",
    "2080Ti * 4\n",
    "Inception v3\n",
    "ILSVRC2012 validation set\n",
    "fp32(original) vs. amp(torch.cuda.amp.autocast)\n",
    "\n",
    "'''\n",
    "    )\n",
    "    torch.cuda.reset_peak_memory_stats(device)\n",
    "\n",
    "    timer = benchmark.Timer(\n",
    "        stmt='inference(model, dataloader, device, False)',\n",
    "        setup='from __main__ import inference',\n",
    "        globals={'model': model, 'dataloader': dataloader, 'device': device},\n",
    "        #num_threads=num_threads,\n",
    "        label='Inference Timing',\n",
    "        sub_label='Original FP32 Inference'\n",
    "    )\n",
    "\n",
    "    s = str(timer.timeit(repeat_times))\n",
    "    result.write(f'{s}\\n')\n",
    "\n",
    "    result.write(f'peak GPU mem usage on active tensors: {torch.cuda.max_memory_allocated(device)/1024.0/1024}MB\\n\\n')\n",
    "\n",
    "    torch.cuda.reset_peak_memory_stats(device)\n",
    "\n",
    "    timer = benchmark.Timer(\n",
    "        stmt='inference(model, dataloader, device, True)',\n",
    "        setup='from __main__ import inference',\n",
    "        globals={'model': model, 'dataloader': dataloader, 'device': device},\n",
    "        #num_threads=num_threads,\n",
    "        label='Inference Timing',\n",
    "        sub_label='Mixed Precision Inference'\n",
    "    )\n",
    "\n",
    "    s = str(timer.timeit(repeat_times))\n",
    "    result.write(f'{s}\\n')\n",
    "\n",
    "    result.write(f'peak GPU mem usage on active tensors: {torch.cuda.max_memory_allocated(device)/1024.0/1024}MB\\n\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.reset_peak_memory_stats(device)\n",
    "inference(model, dataloader, device, False)\n",
    "print(f'peak GPU mem usage on active tensors: {torch.cuda.max_memory_allocated(device)/1024.0/1024}MB\\n')\n",
    "inference(model, dataloader, device, True)\n",
    "print(f'peak GPU mem usage on active tensors: {torch.cuda.max_memory_allocated(device)/1024.0/1024}MB\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test cuda\n",
    "print(torch.cuda.is_available())\n",
    "for i in range(torch.cuda.device_count()):\n",
    "    print(torch.cuda.get_device_name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test dummy cuda inference\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "m = torch.nn.Linear(20, 30).to(DEVICE)\n",
    "input = torch.randn(128, 20).to(DEVICE)\n",
    "output = m(input)\n",
    "print('output', output.size())"
   ]
  }
 ]
}
